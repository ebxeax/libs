{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation Functions\n",
    "Sigmoid/Logstic 可能有梯度离散现象（导数接近0时）\n",
    "##### MSE\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&loss = \\sum[y-(xw+b)]^2\\\\\n",
    "&L2 -norm = ||y-(xw+b)||_2\\\\\n",
    "&loss = norm(y-(xw+b))^2\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Derivative**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&loss = \\sum[y-f_{\\theta}(x)]^2\\\\\n",
    "&\\frac{\\bigtriangledown{loss}}{\\bigtriangledown{\\theta}}=2\\sum[y-f_{\\theta}(x)]*\\frac{\\bigtriangledown{f_{\\theta}(x)}}{\\bigtriangledown{\\theta}}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autograd.grad\n",
    "x = torch.ones(1)\n",
    "w = torch.full([1], 2.)\n",
    "mse = F.mse_loss(torch.ones(1), x*w)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd.grad(y*,[x1, x2, ..., xn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n---------------------------------------------------------------------------\\nRuntimeError                              Traceback (most recent call last)\\n~\\\\AppData\\\\Local\\\\Temp/ipykernel_2932/274727774.py in <module>\\n----> 1 torch.autograd.grad(mse, [w])\\n\\n~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37\\\\lib\\\\site-packages\\torch\\x07utograd\\\\__init__.py in grad(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\\n    234     return Variable._execution_engine.run_backward(\\n    235         outputs, grad_outputs_, retain_graph, create_graph,\\n--> 236         inputs, allow_unused, accumulate_grad=False)\\n    237 \\n    238 \\n\\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.autograd.grad(mse, [w])\n",
    "\"\"\"\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "~\\AppData\\Local\\Temp/ipykernel_2932/274727774.py in <module>\n",
    "----> 1 torch.autograd.grad(mse, [w])\n",
    "\n",
    "~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py in grad(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\n",
    "    234     return Variable._execution_engine.run_backward(\n",
    "    235         outputs, grad_outputs_, retain_graph, create_graph,\n",
    "--> 236         inputs, allow_unused, accumulate_grad=False)\n",
    "    237 \n",
    "    238 \n",
    "\n",
    "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.], requires_grad=True),\n",
       " tensor([2.], requires_grad=True),\n",
       " torch.Size([1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.requires_grad_(), w, w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n---------------------------------------------------------------------------\\nRuntimeError                              Traceback (most recent call last)\\n......\\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.autograd.grad(mse, [w])\n",
    "\"\"\"\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "......\n",
    "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.]),)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = F.mse_loss(torch.ones(1), x*w)\n",
    "torch.autograd.grad(mse, [w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.]), tensor(2.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss.backward\n",
    "mse = F.mse_loss(torch.ones(1), x*w)\n",
    "mse.backward()\n",
    "w.grad, w.grad.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### softmax\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\left[\n",
    "  \\begin{array}{c}\n",
    "          y_1 \\\\\n",
    "          y_2 \\\\\n",
    "          \\vdots \\\\\n",
    "          y_n\n",
    " \\end{array}\n",
    " \\right]\n",
    "&\\rightarrow \\left[S(y_i)=\\frac{e^{y_i}}{\\sum_{j}e^{y_j}}\\right]\\rightarrow \n",
    "  \\left[\n",
    "  \\begin{array}{c}\n",
    "          p_1 \\\\\n",
    "          p_2 \\\\\n",
    "          \\vdots \\\\\n",
    "          p_n\n",
    " \\end{array}\n",
    " \\right]\\\\\n",
    " &\\sum_ip(i)=p(0)+p(1)+\\dots+p(i)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivative**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&p_i=\\frac{e^{a_i}}{\\sum^{N}_{k=1}e^{a_i}} \\qquad & \\green{when\\space i\\neq j}\\\\ \n",
    "&\\frac{\\delta p_i}{\\delta a_j}=\\delta\\frac{\\frac{e^{a_i}}{\\sum^{N}_{k=1}e^{a_k}}}{\\delta a_j} \\qquad  &\\frac{\\delta\\frac{e^{a_i}}{\\sum^{N}_{k=1}e^{a_k}}}{\\delta a_j} =\\frac{e_{a_i}\\sum_{k=1}^{N}e^{a_k}-e^{a_j}e^{a_i}}{(\\sum_{k=1}^{N}e^{a_k})^2}\\\\\n",
    "&f(x)=\\frac{g(x)}{h(x)}  \\qquad &=\\frac{e^{a_i}(\\sum^{N}_{k=1}e^{a_k}-e^{a_j})}{(\\sum_{k=1}^{N}e^{a_k})^2}\\\\\n",
    "&f'(x)=\\frac{g'(x)h(x)-h'(x)g(x)}{h(x)^2} \\qquad &=\\frac{e^{a_j}}{\\sum_{k=1}^{N}e^{a_k}} * \\frac{(\\sum^{N}_{k=1}e^{a_k}-e^{a_j})}{\\sum_{k=1}^{N}e^{a_k}}\\\\\n",
    "&g(x)=e^{a_i} \\qquad &=p_i(1-p_j)\\\\\n",
    "&h(x)=\\sum^{N}_{k=1}e^{a_{i}}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivative**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&p_i=\\frac{e^{a_i}}{\\sum^{N}_{k=1}e^{a_i}} \\qquad & \\green{when\\space i\\neq j}\\\\ \n",
    "&\\frac{\\delta p_i}{\\delta a_j}=\\delta\\frac{\\frac{e^{a_i}}{\\sum^{N}_{k=1}e^{a_k}}}{\\delta a_j} \\qquad  &\\frac{\\delta\\frac{e^{a_i}}{\\sum^{N}_{k=1}e^{a_k}}}{\\delta a_j} =\\frac{0-e^{a_j}e^{a_i}}{(\\sum_{k=1}^{N}e^{a_k})^2}\\\\\n",
    "&f(x)=\\frac{g(x)}{h(x)}   \\qquad &=\\frac{-e^{a_j}}{\\sum_{k=1}^{N}e^{a_k}} * \\frac{e^{a_j}}{\\sum_{k=1}^{N}e^{a_k}}\\\\\n",
    "&f'(x)=\\frac{g'(x)h(x)-h'(x)g(x)}{h(x)^2} \\qquad &=-p_i p_j\\\\\n",
    "&g(x)=e^{a_i}\\\\\n",
    "&h(x)=\\sum^{N}_{k=1}e^{a_{i}}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4549, 0.7119, 0.3149], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3)\n",
    "a.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n---------------------------------------------------------------------------\\nRuntimeError                              Traceback (most recent call last)\\n......\\nRuntimeError: grad can be implicitly created only for scalar outputs\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p = F.softmax(a, dim=0)\n",
    "#p.backward()\n",
    "'''\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "......\n",
    "RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = F.softmax(a, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1293,  0.2417, -0.1124]),)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(p[1], [a], retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0869, -0.1124,  0.1993]),)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(p[2], [a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{\\delta p_i}{\\delta a_j}=\n",
    "\\begin{cases}\n",
    "&p_i(1-p_j)\\space &i=j\\\\\n",
    "&-p_j p_i\\space &i\\neq j\n",
    "\\end{cases}\\\\\n",
    "\n",
    "Or\\space \\space using \\space Kronecker \\space delta &\\delta ij=\n",
    "\\begin{cases}\n",
    "&1 \\space\\space\\space if\\space\\space\\space i=j\\\\\n",
    "&0 \\space\\space\\space if\\space\\space\\space i\\neq j\\\\\n",
    "\\end{cases}\\\\\n",
    "&\\frac{\\delta p_i}{\\delta a_j}=p_i(\\delta_{ij}-p_j)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52608ef383d7879c93a6e97f7ff160bfb17b8cee2f831315816658e0f0556fad"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
